---
title: "Customer personality analysis: Cluster analysis and customer ranking"
author: "Lim Jia Qi"
date: "5/24/2022"
output: 
  prettydoc::html_pretty:
    toc: TRUE
    theme: hpstr
    highlight: vignette
---

# Load library packages
```{r load_library, warning=FALSE}
library(tidyverse)
library(e1071)   # for skewness calculation
require(cluster)  # pam algorithm
library(fpc)      # cluster evaluation metrics
library(mdw)      # entropy-based feature weights
library(MCDA)      # TOPSIS
library(FactoMineR)  # Factor analysis for mixed data (FAMD)
library(factoextra)  
library(knitr)       # For beatiful table display
library(car)         # For interactive 3D scatter plot
library(kableExtra)
```

# Load the previously saved data
```{r load_data}
# set the working directory
setwd("~/ml projects/customer segmentation")
cust_data = readRDS("preprocessed_cust_data.rds")
```

# Cluster analysis

## Dissimilarity matrix computation using gower distance measure

For mixed data types
`daisy()` function from `cluster` package is used to calculate the pairwise dissimilarity between observations. `type` argument in the `daisy` function is used to specify the types of variables in the input `cust_data_without_ID`. More info regarding the `daisy` function can be obtained from the `help()` function.

```{r data_preprocessing}
# Cluster analysis with k-medoids (PAM) algorithm
idx_col_retain = !(colnames(cust_data) %in% c("ID", "min_num_household", "tot_AcceptedCmp"))
cust_data_without_ID = cust_data[,idx_col_retain]

# Boolean attributes
seq_binary = c("Complain", "Response", "Accepted")
idx_binary = sapply(seq_binary, 
                    function(x) grep(x, colnames(cust_data_without_ID)))
idx_binary = unlist(idx_binary)

# continuos attributes
cont_features_patterns = c("Mnt", "Num", "Income","Recency", "age", 
                           "days_enroll") 
idx_cont = sapply(cont_features_patterns, 
                  function(x) grep(x, colnames(cust_data_without_ID)))
idx_cont = unlist(idx_cont)

skewness_col = apply(cust_data_without_ID[, idx_cont], 2, skewness)
idx_logtrans = idx_cont[which(abs(skewness_col)>1)]

# Ordinal attributes

dissimilarity_matrix = daisy(cust_data_without_ID, metric = "gower", 
                             type = list(ordratio=grep("home", colnames(cust_data)),
                                         asymm = idx_binary,
                                         logratio = idx_logtrans))
```

## K-medoid clustering (PAM algorithm)

### Selection of number of clusters, k using cluster evaluation measures
Cluster evaluation can be divided into 3 categories:  

1. Internal: Typical objective functions in clustering formalize the goal of attaining high intra-cluster similarity (data within a cluster are similar) and low inter-cluster similarity (data from different clusters are dissimilar). The similarity Numerical measure of how alike two data instances are normally quantified by distance measures (Euclidean distance).
2. External: Ground truth or gold standard should be available to evaluate how well the clustering results match with the ground truth labels.

I will be using internal cluster evaluation metrics found in `fpc` package, such as *elbow method*, *average silhouette width*, *Calinski-Harabasz index* and *Dunn index* to find the optimal number of cluster.
```{r k_medoid_1}
# Number of clusters are suspected to be in the range of 2 to 8. 
k_array = seq(from = 2, to = 8, by=1)
cluster_eval_df = data.frame(matrix(, nrow = length(k_array), ncol = 4))
colnames(cluster_eval_df) = c("silhouette", "CH_index", "Dunn_index", "wc_SOS")
cluster_eval_df$k = k_array

for (i in (1:length(k_array))){
  set.seed(i+100)
  kmedoid = pam(dissimilarity_matrix, k = k_array[i], diss = TRUE,
                nstart = 10)
  # set diss to TRUE, and set the number of random start as 10.
  clust_stat = cluster.stats(dissimilarity_matrix, 
                             clustering = kmedoid$clustering)
  cluster_eval_df[i,"silhouette"] = clust_stat$avg.silwidth
  cluster_eval_df[i, "CH_index"] = clust_stat$ch
  cluster_eval_df[i, "Dunn_index"] = clust_stat$dunn
  # Add in the within cluster sum of squares
  cluster_eval_df[i, "wc_SOS"] = clust_stat$within.cluster.ss
}

# Line plot for all evaluation metrics (internal cluster evaluation)
cluster_eval_df %>% 
  pivot_longer(!k, names_to = "cluster_eval", values_to = "value") %>% 
  ggplot(aes(x = k, y = value)) +
  geom_line() + geom_point() + facet_grid(rows = vars(cluster_eval), 
                                          scales = "free_y")

# find the best_k from stackoverflow using the mode function found on
# https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode?page=1&tab=scoredesc#tab-top
mode = function(x){
  ux = unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
best_k = mode(c(which.max(cluster_eval_df$silhouette), 
                which.max(cluster_eval_df$CH_index),
                which.max(cluster_eval_df$Dunn_index))) + 1
```

### Final clustering model
```{r k_medoid_2}
set.seed(100 + best_k - 1)
kmedoid = pam(dissimilarity_matrix, k = best_k, diss = TRUE, nstart = 10)
# Save the cluster label in the dataframe. Change it to factor to facilitate data wrangling.
cust_data$cluster_label = as.factor(kmedoid$clustering)
```

## Cluster analysis

### Empirical analysis (Statistical summaries) of each cluster

```{r cluster_analysis_1}
#1: Number of observations (customers)
cust_data %>% group_by(cluster_label) %>% 
  summarise(n = n()) %>% kable(caption = "Number of observations in each cluster")

#2: Average of numerical features for each cluster
summary_cont_features_per_cluster = 
  cust_data %>% group_by(cluster_label) %>% 
  summarise_if(is.numeric, mean) %>% select(-ID)
kable(summary_cont_features_per_cluster, "html", caption = "mean attributes for each cluster") %>% kable_styling("striped") %>% scroll_box(width = "100%")
# can use list() for many variables 


#3: Distribution of categorical features for each cluster
cate_features_names = names(Filter(is.factor, cust_data))
# Change the second argument of group_by() and third argument aes() to   desired categorical feature names: Education, Marital_status, Kidhome,   Teenhome.
cust_data %>% select(one_of(cate_features_names)) %>% 
  group_by(cluster_label, Teenhome) %>% summarise(n = n()) %>% 
  mutate(prop = (n/sum(n))) %>% 
  ggplot(aes(x = cluster_label, y = prop, fill = Teenhome)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_text(aes(label = round(prop, 2)), vjust = 1.5, size = 2,
            position = position_dodge(0.9)) +
  theme_minimal()

#4: Distribution of minimum number of household members for each cluster
cust_data$min_num_household = factor(cust_data$min_num_household, ordered = TRUE)
cust_data %>% group_by(cluster_label, min_num_household) %>% summarise(n = n()) %>% 
  mutate(prop = (n/sum(n))) %>% 
  ggplot(aes(x = cluster_label, y = prop, fill = min_num_household)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_text(aes(label = round(prop, 2)), vjust = 1.5, size = 2,
            position = position_dodge(0.9)) +
  theme_minimal()

#5: Tabulate binary(Boolean) variable for each cluster
table1 = 
  cust_data %>% 
  group_by(cluster_label, Complain) %>% summarise(n = n()) %>% 
  mutate(prop = (n/sum(n))) %>% select(-n)

colnames(table1)[2] = "Binary_outcomes"

#pattern = c("Complain", "Response", "Accept", "cluster")
#idx_select = lapply(pattern, function (x) grep(x, colnames(cust_data)))
#idx = unlist(idx_select)
cust_data_1 = cust_data %>% 
  select(contains("Response") | contains("Accept") | contains("cluster"))

n_col = ncol(cust_data_1)
for (i in (1:(n_col-2))){
  table2 = cust_data_1 %>%  select(c(n_col, all_of(i))) %>% 
    group_by_all() %>% 
    summarise(n = n()) %>% 
    mutate(prop = (n/sum(n))) %>% 
    select(-n)
  colnames(table2)[2] = "Binary_outcomes"
  table1 = table1 %>% 
    left_join(table2, by = c("cluster_label" = "cluster_label", 
                             "Binary_outcomes" = "Binary_outcomes"))
}
                         
oldname = colnames(table1)[3:length(colnames(table1))]
newname = c("Complain", 
            colnames(cust_data_1)[-length(colnames(cust_data_1))])
# Rename
for (i in (1:length(newname))){
  names(table1)[names(table1) == oldname[i]] = newname[i]
}
table1 %>% kable("html", caption = "Proportions of Boolean attributes of each cluster") %>% kable_styling("striped") %>% scroll_box(width = "100%")
```

### Principal component analysis (PCA) for mixed data
Visualize the data in lower dimensional space.
```{r FAMD}
res.famd = FAMD(cust_data_without_ID, graph = F)

eig.value = get_eigenvalue(res.famd)
head(eig.value)
fviz_screeplot(res.famd)
# predict to get the transformed feature space and plot on 3 dimensional scatter plot
transformed_data = predict(res.famd, cust_data_without_ID)
```

The code snippet below is in reference to a [stackoverflow post](https://stackoverflow.com/questions/63595786/rmarkdown-how-to-embed-an-3d-plot)
```{r setup}
options(rgl.useNULL = TRUE)
library(rgl)
```

```{r 3D_plot}
scatter3d(x = transformed_data$coord[,1], y = transformed_data$coord[,2],
          z = transformed_data$coord[,3], 
          groups = cust_data$cluster_label, grid = FALSE, 
          surface = FALSE, ellipsoid = TRUE, 
          surface.col = c("#80FF00", "#009999", "#FF007F"))
rglwidget()
```

**A pop-up window will appear in Rstudio if you run the above code chunk and you can freely rotate the interactive 3D scatter plot. 

# Customer ranking

## TOPSIS
To rank alternatives for each cluster based on features: *Mnt, Num, Accepted and Response*.

### Entropy-based feature weighting

```{r customer_ranking}
# Calculate feature weights for continuous and categorical features for first cluster
i = 1
data_analysis_cont = cust_data %>% filter(cluster_label==i) %>% dplyr::select(starts_with(c("Mnt", "Num")))
data_analysis_cate = cust_data %>% filter(cluster_label==i) %>% 
  dplyr::select(starts_with(c("Accepted", "Response")))

h_cont = get.bw(scale(data_analysis_cont), bw = "nrd", nb = 'na')
w_cont = entropy.weight(scale(data_analysis_cont), h = h_cont)
w_cate = entropy.weight(data_analysis_cate, h='na')
```


### Customer ranking
Customer ranking for 1^st^ cluster.
```{r customer_ranking_2}
data_analysis_cate <- lapply(data_analysis_cate, 
                             function(x) as.numeric(as.character(x)))
data_analysis = cbind(data_analysis_cont, data_analysis_cate)

feat_imp = c(w_cont, w_cate)
overall = TOPSIS(data_analysis, 
                 feat_imp, 
                 criteriaMinMax = rep("max", length(feat_imp)))
data_analysis$topsis_score = overall
data_analysis$ID = cust_data %>% filter(cluster_label==i) %>% select(ID)

# Top 10 customers for cluster 1
data_analysis %>% arrange(desc(topsis_score)) %>% as_tibble() %>% head(10) %>%  relocate(ID, topsis_score) %>% kable("html", caption = "customer ranking according to TOPSIS scores") %>% kable_styling("striped") %>% scroll_box(width = "100%")
```

```{r, echo=FALSE}
R.version
```

